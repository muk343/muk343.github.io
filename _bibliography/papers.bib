---
---


@inproceedings{kumar-etal-2022-empirical,
    title = "An Empirical study to understand the Compositional Prowess of Neural Dialog Models",
    author = "Kumar, Vinayshekhar  and
      Kumar, Vaibhav  and
      Bhutani, Mukul  and
      Rudnicky, Alexander",
    booktitle = "Proceedings of the Third Workshop on Insights from Negative Results in NLP",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.insights-1.21",
    doi = "10.18653/v1/2022.insights-1.21",
    pages = "154--158",
    abstract = "In this work, we examine the problems associated with neural dialog models under the common theme of compositionality. Specifically, we investigate three manifestations of compositionality: (1) Productivity, (2) Substitutivity, and (3) Systematicity. These manifestations shed light on the generalization, syntactic robustness, and semantic capabilities of neural dialog models. We design probing experiments by perturbing the training data to study the above phenomenon. We make informative observations based on automated metrics and hope that this work increases research interest in understanding the capacity of these models.",
    pdf={papers/dialog_composition.pdf}
}

@ARTICLE{https://doi.org/10.48550/arxiv.2303.07675,
  url = {https://arxiv.org/abs/2303.07675},
  author = {Bhutani, Mukul and Kolter, J. Zico},
  keywords = {Machine Learning (cs.LG), Social and Information Networks (cs.SI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Sinkhorn-Flow: Predicting Probability Mass Flow in Dynamical Systems Using Optimal Transport},
  journal = {Optimal Transport Workshop, NeurIPS},  
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license},
  pdf={papers/sinkhorn.pdf},
  abstract={Predicting how distributions over discrete variables vary over time is a common task in time series forecasting. But whereas most approaches focus on merely predicting the distribution at subsequent time steps, a crucial piece of information in many settings is to determine how this probability mass flows between the different elements over time. We propose a new approach to predicting such mass flow over time using optimal transport. Specifically, we propose a generic approach to predicting transport matrices in end-to-end deep learning systems, replacing the standard softmax operation with Sinkhorn iterations. We apply our approach to the task of predicting how communities will evolve over time in social network settings, and show that the approach improves substantially over alternative prediction methods. We specifically highlight results on the task of predicting faction evolution in Ukrainian parliamentary voting.}
}

@inproceedings{gupta-etal-2019-writerforcing,
    title = "{W}riter{F}orcing: Generating more interesting story endings",
    author = "Gupta, Prakhar  and
      Bannihatti Kumar, Vinayshekhar  and
      Bhutani, Mukul  and
      Black, Alan W",
    booktitle = "Proceedings of the Second Workshop on Storytelling",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3413",
    doi = "10.18653/v1/W19-3413",
    pages = "117--126",
    abstract = "We study the problem of generating interesting endings for stories. Neural generative models have shown promising results for various text generation problems. Sequence to Sequence (Seq2Seq) models are typically trained to generate a single output sequence for a given input sequence. However, in the context of a story, multiple endings are possible. Seq2Seq models tend to ignore the context and generate generic and dull responses. Very few works have studied generating diverse and interesting story endings for the same story context. In this paper, we propose models which generate more diverse and interesting outputs by 1) training models to focus attention on important keyphrases of the story, and 2) promoting generating nongeneric words. We show that the combination of the two leads to more interesting endings.",
   pdf={papers/writer_forcing.pdf}
}

@ARTICLE{DBLP:journals/corr/abs-1806-05454,
  author    = {Mukul Bhutani and
               Pratik Jawanpuria and
               Hiroyuki Kasai and
               Bamdev Mishra},
  title     = {Low-rank geometric mean metric learning},
  journal   = {Geometry in Machine Learning (GiMLi) workshop, ICML },
  volume    = {abs/1806.05454},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.05454},
  eprinttype = {arXiv},
  eprint    = {1806.05454},
  timestamp = {Mon, 13 Aug 2018 16:49:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-05454.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  pdf       = {papers/gmml.pdf},
  abstract  = {We propose a low-rank approach to learning a Mahalanobis metric from data. Inspired by the recent geometric mean metric learning (GMML) algorithm, we propose a low-rank variant of the algorithm. This allows to jointly learn a low-dimensional subspace where the data reside and the Mahalanobis metric that appropriately fits the data. Our results show that we compete effectively with GMML at lower ranks.}
}


@InProceedings{10.1007/978-3-319-71273-4_13,
author="Biswas, Arijit
and Bhutani, Mukul
and Sanyal, Subhajit",
title="MRNet-Product2Vec: A Multi-task Recurrent Neural Network for Product Embeddings",
booktitle="ECML PKDD",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="153--165",
abstract="E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell billions of products. Machine learning (ML) algorithms involving products are often used to improve the customer experience and increase revenue, e.g., product similarity, recommendation, and price estimation. The products are required to be represented as features before training an ML algorithm. In this paper, we propose an approach called MRNet-Product2Vec for creating generic embeddings of products within an e-commerce ecosystem. We learn a dense and low-dimensional embedding where a diverse set of signals related to a product are explicitly injected into its representation. We train a Discriminative Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a product title fed through a Bidirectional RNN and at the output, product labels corresponding to fifteen different tasks are predicted. The task set includes several intrinsic characteristics about a product such as price, weight, size, color, popularity, and material. We evaluate the proposed embedding quantitatively and qualitatively. We demonstrate that they are almost as good as sparse and extremely high-dimensional TF-IDF representation in spite of having less than 3{\%} of the TF-IDF dimension. We also use a multimodal autoencoder for comparing products from different language-regions and show preliminary yet promising qualitative results.",
isbn="978-3-319-71273-4",
pdf={papers/mrnet_product2vec.pdf}
}

@ARTICLE{matrixCompletion,
  doi = {10.48550/ARXIV.1711.07684},
  url = {https://arxiv.org/abs/1711.07684},
  author = {Bhutani, Mukul and Mishra, Bamdev},
  title = {A two-dimensional decomposition approach for matrix completion through gossip},
  journal = {Emergent Communication Workshop, NeurIPS },  
  year = {2017},
  archivePrefix = {arXiv},
  copyright = {arXiv.org perpetual, non-exclusive license},
  pdf={papers/matrix_completion_gossip.pdf},
  abstract={Factoring a matrix into two low rank matrices is at the heart of many problems. The problem of matrix completion especially uses it to decompose a sparse matrix into two non sparse, low rank matrices which can then be used to predict unknown entries of the original matrix. We present a scalable and decentralized approach in which instead of learning two factors for the original input matrix, we decompose the original matrix into a grid blocks, each of whose factors can be individually learned just by communicating (gossiping) with neighboring blocks. This eliminates any need for a central server. We show that our algorithm performs well on both synthetic and real datasets.}
}


